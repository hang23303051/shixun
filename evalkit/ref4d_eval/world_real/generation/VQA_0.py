# -*- coding: utf-8 -*-

import os, re, json, math, argparse, warnings, inspect, traceback, gc
from typing import List, Tuple, Dict, Any, Optional

import numpy as np
from PIL import Image

import torch
from transformers import AutoModel, AutoTokenizer

# ================= CLI =================
p = argparse.ArgumentParser()
# 批量模式
p.add_argument('--json-dir',  default='', help='规则 JSON 目录（文件名通常形如 <stem>_rules.json）')
p.add_argument('--video-dir', default='', help='视频目录（与 JSON 同 stem，不同扩展名）')
p.add_argument('--out-dir',   default='', help='输出目录（写出 <video_stem>_VQA.json）')

# ★ 新增：断言目录（可选），用于避免语义相似 VQA
p.add_argument('--assert-dir', default='', help='断言 JSON 目录（按 stem 自动匹配，如 <stem>_assertion*.json）')

# 模型与缓存
p.add_argument('--local-path', default='', help='本地模型目录，如 /data/hf_home/models/openbmb__MiniCPM-V-4_5')
p.add_argument('--model-id',   default='openbmb/MiniCPM-V-4_5')
p.add_argument('--revision',   default='main')
p.add_argument('--cache-dir',  default='')
p.add_argument('--local-files-only', action='store_true')

# 设备/精度/注意力
p.add_argument('--device', default='cuda', choices=['cuda','cpu'])
p.add_argument('--dtype',  default='bf16', choices=['bf16','fp16','fp32'])
p.add_argument('--disable-flash-sdp', dest='disable_flash_sdp', action='store_true')
p.add_argument('--force-math-sdp',    dest='force_math_sdp',    action='store_true')

# 抽帧/解码（整段）
p.add_argument('--fps',           type=int,   default=3,   help='全局抽帧目标 fps')
p.add_argument('--cap-frames',    type=int,   default=300, help='最多送入模型的总帧数上限（全局）')
p.add_argument('--resize-short',  type=int,   default=448, help='短边等比缩放到该尺寸（0=不缩放）')
p.add_argument('--decode-backend', default='auto', choices=['auto','cv2','decord'])

# 生成控制
p.add_argument('--max-new-tokens', type=int, default=512)
p.add_argument('--temperature',    type=float, default=0.0)
p.add_argument('--enable-thinking', action='store_true')
p.add_argument('--verbose', action='store_true')

# 规则入 prompt 数量（按复杂度排序后取前 N 条）
p.add_argument('--max-rules-in-prompt', type=int, default=12)

# 多轮对话控制（默认开启；可用 --single-turn 关闭）
p.add_argument('--multi-turn', dest='multi_turn', action='store_true', help='在回退/补充阶段使用对话历史，让后续问题受前文影响')
p.add_argument('--single-turn', dest='multi_turn', action='store_false',help='禁用对话历史，使用单轮补齐（保持旧行为）')
p.set_defaults(multi_turn=True)

# 原始输出保存
p.add_argument('--dump-raw', action='store_true', help='保存并打印每次模型原始输出（raw）')
p.add_argument('--raw-dir',  default='', help='原始输出保存目录（默认 out-dir/debug/<video_stem>/）')

# ★ 新增：与断言语义去重阈值
p.add_argument('--assert-jaccard-thresh', type=float, default=0.70, help='与断言语义相似的去重阈值（0~1，越大越严格）')

# ★ 新增：每个视频处理后是否强制重载模型（默认 False；显存非常紧张或第三方库存在缓存泄漏时可开启）
p.add_argument('--reload-per-video', action='store_true', help='每个视频处理前重载模型，处理后释放，最大化显存回收')

args = p.parse_args()

# ================= 常量/工具 =================
TIME_SCALE = 0.1
MAX_NUM_FRAMES = 450
MAX_NUM_PACKING = 3

# 输出 SCHEMA（示例说明；实际产出字段名见 _assemble_results）
SCHEMA_STR = """
[
  {
    "id": "v1",
    "vqa_text": "Does the object remain supported by the base throughout the clip?",
    "required_singal": ["support(oX)"]
  }
]
""".strip()

VIDEO_EXTS = {'.mp4', '.mkv', '.mov', '.avi', '.m4v', '.webm'}

def _strip_think(text: str) -> str:
    if not isinstance(text, str):
        return text
    return re.sub(r'<think>.*?</think>', '', text, flags=re.S|re.I).strip()

def _strip_code_fences_keep_inner(text: str) -> str:
    """
    去掉代码围栏标记（```xxx 与 ```），保留内部文字。
    同时剔除纯围栏行（只含```或```lang）。
    """
    if not isinstance(text, str):
        text = str(text)
    lines = text.replace('\r', '\n').split('\n')
    out_lines = []
    fence = False
    for ln in lines:
        if re.match(r'^\s*```', ln):
            fence = not fence
            continue  # 丢弃围栏标记行
        out_lines.append(ln)
    return '\n'.join(out_lines)

def _dtype_of(device: str, choice: str):
    if device == 'cpu':
        return torch.float32
    return {'bf16': torch.bfloat16, 'fp16': torch.float16, 'fp32': torch.float32}[choice]

def _dtype_kw(d):
    sig = inspect.signature(AutoModel.from_pretrained)
    return {"dtype": d} if "dtype" in sig.parameters else {"torch_dtype": d}

def _read_json(fp: str) -> Dict[str, Any]:
    with open(fp, 'r', encoding='utf-8') as f:
        return json.load(f)

def _ensure_dir_dir(dir_path: str):
    if dir_path:
        os.makedirs(dir_path, exist_ok=True)

def _ensure_dir_for_file(file_path: str):
    os.makedirs(os.path.dirname(file_path) or ".", exist_ok=True)

# ================= 视频读取与抽帧 =================
def read_video_meta_and_backend(video_path: str, preference: str = 'auto'):
    cap = None; vr = None
    backend = None; fps = None; total = None; duration = None

    if preference in ('auto', 'cv2'):
        try:
            import cv2
            cap = cv2.VideoCapture(video_path)
            total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
            fps = float(cap.get(cv2.CAP_PROP_FPS) or 25.0)
            duration = total / max(fps, 1e-6) if total > 0 else 0.0
            if total > 0:
                backend = 'cv2'
                return backend, (cap, None), fps, total, duration
        except Exception:
            pass

    if preference in ('auto', 'decord'):
        try:
            from decord import VideoReader, cpu
            vr = VideoReader(video_path, ctx=cpu(0))
            total = len(vr)
            try:
                fps = float(vr.get_avg_fps())
            except Exception:
                fps = 25.0
            duration = total / max(fps, 1e-6)
            backend = 'decord'
            return backend, (None, vr), fps, total, duration
        except Exception:
            pass

    raise RuntimeError(f"No available video backend for {video_path} (tried cv2, decord)")

def _release_video_handles(backend: Optional[str], handles: Any):
    """在抽帧完成后尽早释放视频句柄，避免文件/内存占用。"""
    try:
        if backend == 'cv2' and handles and handles[0] is not None:
            cap, _ = handles
            try:
                cap.release()
            except Exception:
                pass
        elif backend == 'decord' and handles and handles[1] is not None:
            _, vr = handles
            # decord 没有显式 close，删除引用并交给 GC
            del vr
    except Exception:
        pass

def read_frames(backend: str, handles, idx_all: np.ndarray) -> List[Image.Image]:
    frames: List[Image.Image] = []
    if backend == 'cv2':
        cap, _ = handles
        import cv2
        if len(idx_all) == 0:
            return frames
        try:
            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx_all[0]))
        except Exception:
            pass
        cur = int(idx_all[0])
        want = set(int(i) for i in idx_all.tolist())
        while True:
            ok, frame = cap.read()
            if not ok:
                break
            if cur in want:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(Image.fromarray(frame))
                if len(frames) >= len(idx_all):
                    break
            cur += 1
    else:
        _, vr = handles
        if len(idx_all) == 0:
            return frames
        arr = vr.get_batch(idx_all.tolist()).asnumpy()
        frames = [Image.fromarray(x.astype('uint8')).convert('RGB') for x in arr]
    return frames

def _temporal_ids(ts: np.ndarray) -> List[List[int]]:
    grid = np.round(ts / TIME_SCALE).astype(np.int32)
    n = len(grid)
    packing = min(MAX_NUM_PACKING, max(1, int(math.ceil(n / MAX_NUM_FRAMES))))
    chunks = []
    for k in range(packing):
        part = grid[k::packing]
        chunks.append(part.tolist())
    return chunks

def encode_whole_video(meta, choose_fps: int, cap_frames: int,
                       resize_short: int) -> Tuple[List[Image.Image], List[List[int]], Tuple[float, float], int]:
    backend, handles, fps, total, duration = meta
    s, e = 0.0, duration
    clip_len = max(1e-6, e - s)

    want = int(min(cap_frames, round(clip_len * min(choose_fps, int(round(fps))))))
    start_idx = 0
    end_idx   = max(0, total - 1)
    if end_idx <= start_idx:
        end_idx = min(total - 1, start_idx + max(1, want))

    idx_all = np.linspace(start_idx, end_idx, max(1, want)).astype(np.int64)
    ts = idx_all / max(fps, 1e-6)

    frames = read_frames(backend, handles, idx_all)

    # 读完帧就释放视频句柄，降低常驻内存占用
    _release_video_handles(backend, handles)

    if resize_short and resize_short > 0:
        nf = []
        for img in frames:
            w, h = img.size
            short = min(w, h)
            if short > resize_short:
                if w < h:
                    new_w = resize_short
                    new_h = int(h * (resize_short / w))
                else:
                    new_h = resize_short
                    new_w = int(w * (resize_short / h))
                img = img.resize((new_w, new_h), Image.BILINEAR)
            nf.append(img)
        frames = nf

    tids = _temporal_ids(ts)
    return frames, tids, (s, e), len(idx_all)

# ================= 模型加载 / 释放 =================
def load_model(where: str, device: str, dtype, cache_dir: str, local_only: bool, revision: str, is_dir: bool):
    common = dict(trust_remote_code=True, attn_implementation='sdpa', **_dtype_kw(dtype))
    if cache_dir:
        common['cache_dir'] = cache_dir
    if local_only:
        common['local_files_only'] = True

    if is_dir:
        m = AutoModel.from_pretrained(where, **common).eval()
        t = AutoTokenizer.from_pretrained(where, trust_remote_code=True, local_files_only=local_only)
    else:
        m = AutoModel.from_pretrained(where, revision=revision, **common).eval()
        t = AutoTokenizer.from_pretrained(
            where, revision=revision, trust_remote_code=True,
            **({} if not cache_dir else {'cache_dir': cache_dir})
        )

    if device == 'cuda' and torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        try:
            if args.disable_flash_sdp:
                torch.backends.cuda.enable_flash_sdp(False)
            if args.force_math_sdp:
                torch.backends.cuda.enable_math_sdp(True)
                torch.backends.cuda.enable_mem_efficient_sdp(False)
            else:
                torch.backends.cuda.enable_math_sdp(True)
                torch.backends.cuda.enable_mem_efficient_sdp(True)
        except Exception:
            pass
        m = m.to('cuda')

    return m, t

def _flush_model_caches(model):
    """
    温和地尝试清空模型内部缓存（image cache / kv cache / past key values 等），
    不依赖具体实现，尽最大可能释放。
    """
    try:
        # 常见自定义接口
        if hasattr(model, "clear_image_cache") and callable(model.clear_image_cache):
            model.clear_image_cache()
    except Exception:
        pass
    try:
        if hasattr(model, "reset_cache") and callable(model.reset_cache):
            model.reset_cache()
    except Exception:
        pass
    try:
        if hasattr(model, "flush_caches") and callable(model.flush_caches):
            model.flush_caches()
    except Exception:
        pass
    # 兜底：尝试清空通用属性
    for attr in ("image_cache", "kv_cache", "past_key_values", "_past_key_values"):
        try:
            cache = getattr(model, attr, None)
            if isinstance(cache, dict):
                cache.clear()
            elif cache is not None:
                setattr(model, attr, None)
        except Exception:
            pass

def _finalize_gpu_step():
    """统一的 GPU/CPU 释放流程：清 Python 引用 + CUDA 缓存 + GC。"""
    try:
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
            # 收集 IPC 共享块，进一步归还
            torch.cuda.ipc_collect()
    except Exception:
        pass
    gc.collect()

# ================= 规则打分（用于筛选入 prompt 的规则） =================
_SIGNAL_WEIGHT = {
  "interpenetration": 2.7,
  "collision": 2.2,
  "post_collision_change": 2.5,
  "center_of_mass_over_base": 2.3,
  "shadow_consistency": 2.8,
  "size_inconsistency": 2.9,
  "reflection_consistency": 2.6,
  "biological_motion": 3.0,
  "material_optics": 2.9,
  "deformation": 3.2,
  "fracture": 2.2,
  "support": 2.6,
  "balance": 2.7,
  "trajectory_dir": 3.0,
  "accel_sign": 2.6,
  "rigid_transform": 2.9,
  "tilt_angle_threshold": 2.7,
  "flame_proximity": 2.5,
  "cast_shadow_contact": 2.9,
  "friction_sliding": 2.0,
  "wheel_rolling": 2.0,
  "hue_histogram_jump": 2.4,
  "hot_surface_contact": 2.5,
  "slosh_response": 2.5,
  "buoyancy_float": 2.5,
  "specular_highlight_consistency": 2.8,
  "smoke_direction_consistency": 2.8,
  "frame_discontinuity": 3.3,
  "continuity": 2.8,
  "compressibility": 2.1,
  "leakage": 1.9,
  "grasp_stability": 1.8,
  "tool_contact": 1.8,
  "spillage": 1.8,
  "viscosity_effect": 2.5,
  "pour_direction": 1.8,
  "slip": 1.7,
  "step_order": 1.6,
  "handoff_between_hands": 1.6,
  "state_change": 2.9,
  "tool_usage_correct": 1.9,
  "goal_placement": 1.6,
  "result_appearance": 1.8,
  "action_phase": 1.6,
  "supportable": 1.6,
  "placeable": 1.5,
  "pourable": 1.5,
  "openable": 1.5,
  "containment": 1.9,
  "prohibited_intersection": 2.8,
  "falling_hazard": 2.2,
  "finger_clearance": 2.5,
  "blade_orientation_away": 1.9,
  "_default": 1.2
}

def _prefix(sig: str) -> str:
    s = (sig or '').strip().lower()
    s = re.split(r'[\(\s]', s)[0]
    s = s.replace('/', '_').replace('-', '_').replace(' ', '_').replace('.', '_')
    s = re.sub(r'[^a-z0-9_]', '_', s)
    return s or '_default'

def _rule_quality(rule: Dict[str, Any]) -> float:
    sigs = rule.get('required_signals') or []
    if not isinstance(sigs, list):
        return 0.0
    score = 0.0
    for s in sigs:
        key = _prefix(s)
        score += _SIGNAL_WEIGHT.get(key, _SIGNAL_WEIGHT['_default'])
    score += 0.15 * max(0, len(sigs) - 1)
    kinds = set(_prefix(s).split('_')[0] for s in sigs if isinstance(s, str))
    score += 0.1 * max(0, len(kinds) - 1)
    return score

def _select_rules(rule_json: Dict[str, Any], k: int) -> List[Dict[str, Any]]:
    all_rules = [r for r in (rule_json.get('rules') or []) if isinstance(r, dict)]
    for r in all_rules:
        r['_quality'] = _rule_quality(r)
    all_rules.sort(key=lambda r: r.get('_quality', 0.0), reverse=True)
    return all_rules[:k]

# ================= 提示构造（仅传 id + rule_text；输出需贴 [rX]） =================
BASE_INSTRUCTION = """
Please generate VQA (Video Question Answering) questions that are STRICTLY grounded in the given reference video, and whose short answers—derivable from the video—are TRUE or demonstrably PRESENT in that reference video.

[Background]
Using the high-level rules below (ranked from higher to lower importance) together with the entire reference video, write verifiable questions to test common sense, physical laws, and causal coherence (e.g., no sudden limb twisting, consistent lighting/shadow direction, stable camera/viewpoint, no object interpenetration, physically plausible motion, etc.). Each question must be answerable by watching the video alone, and the answer must be short (e.g., Yes/No, a word/phrase, a color, a count, a direction, whether an event occurs, etc.).

[Clip] {CLIP_RANGE} (global)

[Available Rules (higher-ranked preferred)]
{RULES_RANKED_BLOCK}
Note: Each line is in the form “- rID: <rule_text>”. At the start of each question line, cite one or more rule ids using [rID] tags.

[Avoid Overlap with Assertions]
Do NOT write questions that paraphrase or are semantically similar to the following assertion texts:
{ASSERTIONS_AVOID_BLOCK}

[Hard Output Requirements]
Output at most FIVE lines.
Each line must begin with one or more rule id tags, followed by ONE English question:
[rID][rID2] question_text
The question must be directly answerable from the video with a SHORT answer, and that answer must be TRUE for this reference video.
Do NOT include extra descriptions, titles, numbering, blank lines, code fences, or <think> tags.

[Quality Requirements]
VQA questions must be specific and answerable, focusing on world common sense/physics, states, and reasonableness of size/scale.
Strictly video-grounded: refer only to entities/events clearly visible in the video; no hypotheticals or external knowledge.
Prefer higher-ranked rules or combinations; avoid repetition and paraphrase-only variants.
Keep questions concise, unambiguous, and easy to verify; use simple present tense.
For Yes/No questions, frame them so that the TRUE answer in this reference video is clear (e.g., ask for the presence of correct behavior, or the absence of anomalies actually observed).

[Recommended Patterns (use as needed)]
[rID]“Does <object> stay supported by <surface> without floating or sinking?”
[rID][rID2]“Across the clip, does <object> keep a consistent size relative to <reference>?”
""".strip()

def _compact_rules_for_prompt(rule_json: Dict[str, Any]) -> Tuple[str, Dict[str, Dict[str, Any]]]:
    rules = rule_json.get("rules", []) or []
    id_map: Dict[str, Dict[str, Any]] = {}
    for r in rules:
        rid = str(r.get("id") or "").strip()
        if not rid:
            continue
        id_map[rid] = {
            "id": rid,
            "rule_text": r.get("rule_text"),
            "required_signals": r.get("required_signals") or []
        }
    k = min(args.max_rules_in_prompt, len(id_map))
    chosen = _select_rules({"rules": list(id_map.values())}, k)
    lines = [f"- {r.get('id')}: {r.get('rule_text')}" for r in chosen]
    block = "\n".join(lines)
    return block, id_map

def _format_assertions_block(assert_texts: List[str], limit: int = 12) -> str:
    if not assert_texts:
        return "(none)"
    items = [f"- {t}" for t in assert_texts[:limit]]
    if len(assert_texts) > limit:
        items.append(f"- (+{len(assert_texts)-limit} more)")
    return "\n".join(items)

def build_user_prompt_from_rules(rule_json: Dict[str, Any],
                                 clip_info: Tuple[float, float],
                                 assert_texts: List[str]) -> Tuple[str, Dict[str, Dict[str, Any]]]:
    s, e = clip_info
    rules_block, id_map = _compact_rules_for_prompt(rule_json)
    prompt = BASE_INSTRUCTION.format(
        CLIP_RANGE=f"{s:.1f}-{e:.1f}s",
        RULES_RANKED_BLOCK=rules_block,
        ASSERTIONS_AVOID_BLOCK=_format_assertions_block(assert_texts)
    )
    return prompt, id_map

# ================= RAW 输出小工具 =================
def _default_raw_dir_for_video(out_dir: str, video_path: str) -> str:
    stem = os.path.splitext(os.path.basename(video_path))[0]
    return os.path.join(out_dir, "debug", stem)

def _save_raw(text: str, path: str, head_print: int = 2000, tag: str = "RAW OUT"):
    try:
        _ensure_dir_for_file(path)
        with open(path, "w", encoding="utf-8") as f:
            f.write(text if isinstance(text, str) else str(text))
    except Exception:
        pass
    print(f"[{tag}] {text[:head_print]}{'...' if isinstance(text, str) and len(text)>head_print else ''}", flush=True)

# ================= 解析 [rX] 标签行 =================
_TAG_RE = re.compile(r'\[(r[0-9a-zA-Z_\-]+)\]')

def _strip_answer_markers(s: str) -> str:
    s = re.sub(r'\b(answer|ans|a)\s*[:\-]\s*', '', s, flags=re.I)
    s = re.sub(r'\b(yes|no)\s*[:\-]\s*', '', s, flags=re.I)
    return s.strip()

def _truncate_to_first_question(s: str) -> str:
    qpos = s.find('?')
    if qpos != -1:
        return s[:qpos+1].strip()
    return s.strip()

def _normalize_tokens(s: str) -> List[str]:
    s = s.lower()
    s = re.sub(r'[^a-z0-9\s\-_/]', ' ', s)
    toks = [t for t in s.split() if t]
    return toks

def _jaccard(a_tokens: List[str], b_tokens: List[str]) -> float:
    A, B = set(a_tokens), set(b_tokens)
    if not A or not B:
        return 1.0 if ' '.join(a_tokens).strip() == ' '.join(b_tokens).strip() else 0.0
    return len(A & B) / max(1, len(A | B))

def _is_semantic_duplicate(a: str, b: str, jaccard_thresh: float = 0.8) -> bool:
    return _jaccard(_normalize_tokens(a), _normalize_tokens(b)) >= jaccard_thresh

def _clean_text_before_parse(text: str) -> str:
    t = _strip_think(text)
    t = _strip_code_fences_keep_inner(t)
    return t

def _parse_tagged_lines(text: str) -> List[Tuple[List[str], str]]:
    """
    输入模型原始文本，解析出若干行：
      返回 [(rule_ids, question_text), ...]
    行允许有前缀编号，如 "1. "、"1、"、"① " 等，都会被忽略。
    """
    if not isinstance(text, str):
        text = str(text)
    text = _clean_text_before_parse(text)

    lines = [ln.strip() for ln in text.replace('\r', '\n').split('\n') if ln.strip()]
    out: List[Tuple[List[str], str]] = []
    for ln in lines:
        ln = re.sub(r'^\s*(\d+[\.\)]|[（(]?\d+[）)]|[①-⑩]|[一二三四五六七八九十]\s*[、.])\s*', '', ln)
        if ln.strip().lower().startswith(('json', 'output:', 'answer:', 'assertions:', 'result:')):
            continue
        tags = _TAG_RE.findall(ln)
        if not tags:
            continue
        body = _TAG_RE.sub('', ln).strip()
        body = re.sub(r'^[\-\:：、.\s]+', '', body).strip()
        body = _strip_answer_markers(body)
        body = _truncate_to_first_question(body)
        if not body or not body.endswith('?'):
            continue
        uniq_ids: List[str] = []
        seen = set()
        for t in tags:
            if t not in seen:
                uniq_ids.append(t)
                seen.add(t)
        out.append((uniq_ids, body))
    return out

# ================= 断言加载/匹配 =================
def _basename_no_ext(path: str) -> str:
    return os.path.splitext(os.path.basename(path))[0]

def _drop_rules_suffix(name: str) -> str:
    if name.lower().endswith('_rules'):
        return name[: -len('_rules')]
    return name

def _video_candidates(video_dir: str) -> Dict[str, str]:
    cand: Dict[str, str] = {}
    for fn in os.listdir(video_dir):
        full = os.path.join(video_dir, fn)
        if not os.path.isfile(full):
            continue
        ext = os.path.splitext(fn)[1].lower()
        if ext not in VIDEO_EXTS:
            continue
        stem = _basename_no_ext(fn)
        key = stem.lower()
        if key not in cand or len(fn) < len(os.path.basename(cand[key])):
            cand[key] = full
    return cand

def _best_match_video(video_dir: str, rule_stem: str) -> Tuple[Optional[str], Optional[str]]:
    cand = _video_candidates(video_dir)
    target = rule_stem.lower()
    if target in cand:
        vp = cand[target]
        return vp, _basename_no_ext(vp)
    prefix_hits = [(k, v) for k, v in cand.items() if k.startswith(target)]
    if prefix_hits:
        prefix_hits.sort(key=lambda kv: (len(kv[0]), len(os.path.basename(kv[1]))))
        vp = prefix_hits[0][1]
        return vp, _basename_no_ext(vp)
    contain_hits = [(k, v) for k, v in cand.items() if target in k]
    if contain_hits:
        contain_hits.sort(key=lambda kv: (len(kv[0]), len(os.path.basename(kv[1]))))
        vp = contain_hits[0][1]
        return vp, _basename_no_ext(vp)
    return None, None

def _assert_candidates(assert_dir: str) -> Dict[str, str]:
    cand: Dict[str, str] = {}
    if not assert_dir or not os.path.isdir(assert_dir):
        return cand
    for fn in os.listdir(assert_dir):
        full = os.path.join(assert_dir, fn)
        if not os.path.isfile(full) or not fn.lower().endswith('.json'):
            continue
        stem = _basename_no_ext(fn).lower()
        # 常见命名：<stem>_assertion.json / <stem>_assertions.json / 其它
        cand[stem] = full
    return cand

def _best_match_assert(assert_dir: str, stem: str) -> Optional[str]:
    """
    规则：先找完全相等，再找前缀匹配，再找包含匹配；
    同时尝试 _assertion / _assertions 后缀的变体。
    """
    if not assert_dir or not os.path.isdir(assert_dir):
        return None
    cand = _assert_candidates(assert_dir)
    target = stem.lower()
    # 直接命中
    if target in cand:
        return cand[target]
    # 常见后缀
    for suf in ('_assertion', '_assertions'):
        key = (target + suf)
        if key in cand:
            return key and cand[key]
    # 前缀
    prefix_hits = [(k, v) for k, v in cand.items() if k.startswith(target)]
    if prefix_hits:
        prefix_hits.sort(key=lambda kv: (len(kv[0]), len(os.path.basename(kv[1]))))
        return prefix_hits[0][1]
    # 包含
    contain_hits = [(k, v) for k, v in cand.items() if target in k]
    if contain_hits:
        contain_hits.sort(key=lambda kv: (len(kv[0]), len(os.path.basename(kv[1]))))
        return contain_hits[0][1]
    return None

def _extract_assert_texts(obj: Any) -> List[str]:
    """
    尽量鲁棒地从多种结构提取断言文本：
    - [{ "assertion_text": "...", "required_signal": [...] }, ...]
    - [{ "assention_text": "..." }, ...]   # 兼容拼写
    - [{ "name": "..." }, ...] 或 [{ "text": "..." }, ...]
    - { "assertions": [ ... 如上 ... ] }
    - ["...", "..."]
    """
    txts: List[str] = []
    def _maybe_add(x: Any):
        if not x: return
        s = str(x).strip()
        if s:
            txts.append(s)

    if isinstance(obj, dict) and 'assertions' in obj:
        obj = obj.get('assertions')

    if isinstance(obj, list):
        for it in obj:
            if isinstance(it, str):
                _maybe_add(it)
            elif isinstance(it, dict):
                for key in ('assertion_text', 'assention_text', 'name', 'text', 'assention', 'assertion'):
                    if key in it and isinstance(it[key], (str, int, float)):
                        _maybe_add(it[key])
                        break
    elif isinstance(obj, dict):
        # 尝试把所有字符串 value 拉出来
        for v in obj.values():
            if isinstance(v, (str, int, float)):
                _maybe_add(v)
            elif isinstance(v, list):
                for e in v:
                    if isinstance(e, (str, int, float)):
                        _maybe_add(e)
                    elif isinstance(e, dict):
                        for key in ('assertion_text','assention_text','name','text'):
                            if key in e and isinstance(e[key], (str, int, float)):
                                _maybe_add(e[key])
                                break
    return txts

# ================= 将解析结果组装为目标 JSON =================
def _assemble_results(tagged: List[Tuple[List[str], str]],
                      rules_by_id: Dict[str, Dict[str, Any]],
                      *,
                      start_index: int = 1,
                      dedup: bool = True,
                      assert_texts: Optional[List[str]] = None,
                      assert_jaccard_thresh: float = 0.70,
                      verbose: bool = False) -> List[Dict[str, Any]]:
    """
    不再自动补齐到 5 条；只返回实际拿到的条目（最多 5 条）。
    额外：丢弃与断言相似的问题。
    """
    result: List[Dict[str, Any]] = []
    texts_so_far: List[str] = []
    assert_texts = assert_texts or []

    for ids, body in tagged:
        body_clean = body.strip()
        if not body_clean:
            continue

        # 与已收集问题去重
        if dedup:
            dup = False
            for prev in texts_so_far:
                if _is_semantic_duplicate(prev, body_clean):
                    dup = True
                    break
            if dup:
                if verbose:
                    print(f"[Drop: duplicate-with-VQA] {body_clean}")
                continue

        # 与断言去重（关键增强）
        conflict = False
        for at in assert_texts:
            if _is_semantic_duplicate(at, body_clean, jaccard_thresh=assert_jaccard_thresh):
                conflict = True
                break
        if conflict:
            if verbose:
                print(f"[Drop: similar-to-assertion] {body_clean}")
            continue

        # 汇总 required_singal
        signals: List[str] = []
        sig_seen = set()
        for rid in ids:
            r = rules_by_id.get(rid)
            if not r:
                continue
            for s in (r.get("required_signals") or []):
                s_str = str(s)
                if s_str not in sig_seen:
                    signals.append(s_str)
                    sig_seen.add(s_str)

        result.append({
            "id": "",            # 稍后规范化 v1..vK
            "vqa_text": body_clean,
            "required_singal": signals
        })
        texts_so_far.append(body_clean)
        if len(result) >= 5:
            break

    for i, it in enumerate(result, start_index):
        it["id"] = f"v{i}"

    return result[:5]

# ================= 主流程 =================
def _process_one(rule_json_path: str, video_path: str, out_json_path: str,
                 model, tok, args, assert_path: Optional[str]):
    backend, handles, fps, total, duration = read_video_meta_and_backend(
        video_path, preference=args.decode_backend
    )
    if args.verbose:
        print(f"[Video] {os.path.basename(video_path)} | {duration:.2f}s | fps={fps:.2f} | frames={total} | backend={backend}")

    rulej = _read_json(rule_json_path)

    # 读取断言文本（可选）
    assert_texts: List[str] = []
    if assert_path and os.path.isfile(assert_path):
        try:
            aj = _read_json(assert_path)
            assert_texts = _extract_assert_texts(aj)
            if args.verbose:
                print(f"[Assert] loaded {len(assert_texts)} texts from {os.path.basename(assert_path)}")
        except Exception as e:
            print(f"[Warn] Failed to read assertions from {assert_path}: {e}")

    # 全局抽帧（抽完立即释放视频句柄）
    frames, tids, (cs, ce), _ = encode_whole_video(
        (backend, handles, fps, total, duration),
        args.fps, min(args.cap_frames, MAX_NUM_FRAMES),
        args.resize_short
    )

    # 构造提示（加入断言避免块）
    user_prompt, rules_by_id_all = build_user_prompt_from_rules(rulej, (cs, ce), assert_texts)

    # 原始输出目录
    raw_dir = args.raw_dir or _default_raw_dir_for_video(args.out_dir, video_path)
    if args.dump_raw:
        _ensure_dir_dir(raw_dir)

    # ============ 小工具：构造 user 消息（首轮带帧；后续仅文本） ============
    def _make_user_msg(prompt: str, *, include_frames: bool = True) -> Dict[str, Any]:
        return {'role': 'user', 'content': (frames + [prompt]) if include_frames else [prompt]}

    # ============ 调模型：控制是否复用图像缓存 use_image_id ============
    def _call_model(msgs: List[Dict[str, Any]], *, tag: str, round_idx: int = 1,
                    max_new_tokens: Optional[int] = None, use_image_id: bool = False) -> str:
        with torch.inference_mode():
            out = model.chat(
                msgs=msgs,
                tokenizer=tok,
                temporal_ids=tids,
                use_image_id=use_image_id,
                max_slice_nums=1,
                do_sample=False,
                temperature=None,
                enable_thinking=False,
                max_new_tokens=(max_new_tokens if max_new_tokens is not None else args.max_new_tokens)
            )
        raw = out if isinstance(out, str) else str(out)
        if args.dump_raw:
            fn = "raw_first.txt" if tag == "first" and round_idx == 1 else f"raw_round_{round_idx:02d}.txt"
            _save_raw(raw, os.path.join(raw_dir, fn), tag=f"RAW OUT {tag.upper()}#{round_idx}")
        return _clean_text_before_parse(raw)

    multi_turn = getattr(args, "multi_turn", True)

    # ==================== 首轮（带帧，use_image_id=False） ====================
    msgs: List[Dict[str, Any]] = []
    msgs.append(_make_user_msg(user_prompt, include_frames=True))
    text_first = _call_model(msgs, tag="first", round_idx=1, use_image_id=False)
    tagged = _parse_tagged_lines(text_first)

    # 把助手输出加入历史
    msgs.append({'role': 'assistant', 'content': text_first})

    # ============== 多轮对话式回退/补齐 ==============
    RETRY_LIMIT = 5
    round_idx = 1

    def _build_continue_prompt(current_tagged: List[Tuple[List[str], str]], want: int, assert_texts: List[str]) -> str:
        exist_texts = [t for _, t in current_tagged]
        exist_block = "\n".join(f"- {t}" for t in exist_texts) if exist_texts else "(none)"
        avoid_block = _format_assertions_block(assert_texts)
        return (
            "Continue. Based on the previous assistant output above and the SAME global video, "
            "output ONLY NEW VQA questions. Each line MUST start with rule tags like [r1][r5] "
            "followed by an ENGLISH QUESTION that ends with '?'. DO NOT answer.\n"
            "Avoid overlap, paraphrases, or minor rewording of earlier questions.\n"
            "ALSO avoid paraphrasing or being semantically similar to the following assertions:\n"
            f"{avoid_block}\n\n"
            f"[Already generated questions (DO NOT repeat)]:\n{exist_block}\n\n"
            f"Output up to {want} new lines. No JSON, no explanations, no code fences."
        )

    while len(tagged) < 5 and round_idx < RETRY_LIMIT:
        round_idx += 1
        remain = 5 - len(tagged)

        if multi_turn:
            cont_prompt = _build_continue_prompt(tagged, remain, assert_texts)
            msgs.append(_make_user_msg(cont_prompt, include_frames=False))  # 后续仅文本
            text_more = _call_model(
                msgs, tag="dialogue", round_idx=round_idx,
                max_new_tokens=args.max_new_tokens + 128,
                use_image_id=True  # 复用首轮图像
            )
            msgs.append({'role': 'assistant', 'content': text_more})
        else:
            exist_texts = [t for _, t in tagged]
            exist_block = "\n".join(f"- {t}" for t in exist_texts) if exist_texts else "(none)"
            refill_prompt = (
                user_prompt
                + "\n\n[Avoid paraphrasing these assertions again.]\n"
                + _format_assertions_block(assert_texts)
                + "\n\n[Already generated questions (DO NOT repeat; avoid paraphrases)]:\n"
                + exist_block
                + f"\n\nNow output ONLY NEW lines. Each line MUST start with rule tags like [r1][r5] "
                  "followed by an ENGLISH QUESTION that ends with '?'. DO NOT answer. "
                  "DO NOT include JSON, explanations, or code fences. "
                  f"Output up to {remain} lines."
            )
            msgs_single = [_make_user_msg(refill_prompt, include_frames=True)]
            text_more = _call_model(
                msgs_single, tag="retry", round_idx=round_idx,
                max_new_tokens=args.max_new_tokens + 128,
                use_image_id=False
            )

        add_tagged = _parse_tagged_lines(text_more)

        # 语义去重（与已有问题）
        def _already_has(body: str) -> bool:
            for _, b in tagged:
                if _is_semantic_duplicate(b, body):
                    return True
            return False

        for ids, body in add_tagged:
            if not _already_has(body):
                tagged.append((ids, body))

        if args.verbose:
            print(f"[Retry/Dialogue] round={round_idx}, now_have={len(tagged)}", flush=True)

    # 组装为最终 JSON（不强行补齐；最多 5 条；required_singal 按 [rX] 汇总）
    result = _assemble_results(
        tagged, rules_by_id_all, start_index=1, dedup=True,
        assert_texts=assert_texts,
        assert_jaccard_thresh=args.assert_jaccard_thresh,
        verbose=args.verbose
    )

    _ensure_dir_for_file(out_json_path)
    with open(out_json_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    if args.verbose:
        print(f"[OK] -> {out_json_path} | items={len(result)}")

    # ========= 每个视频结束后的释放逻辑（关键） =========
    try:
        # 清空模型内部缓存（尤其是 image cache / kv cache）
        _flush_model_caches(model)
    except Exception:
        pass

    # 删除大对象引用，帮助 GC 回收
    try:
        del frames, tids, msgs, text_first, tagged, result, rulej, assert_texts, rules_by_id_all
    except Exception:
        pass

    _finalize_gpu_step()
    return  # 明确返回，结束该视频作用域

def main():
    if not args.json_dir or not args.video_dir or not args.out_dir:
        raise ValueError("需要提供 --json-dir、--video-dir、--out-dir")

    _ensure_dir_dir(args.out_dir)

    device = 'cuda' if (args.device == 'cuda' and torch.cuda.is_available()) else 'cpu'
    dtype = _dtype_of(device, args.dtype)

    where = args.local_path if args.local_path else args.model_id
    is_dir = bool(args.local_path and os.path.isdir(args.local_path))

    # 如果不 reload-per-video：这里加载一次；否则在循环内部按需重载
    model, tok = (None, None)
    if not args.reload_per_video:
        model, tok = load_model(where, device, dtype, args.cache_dir, args.local_files_only, args.revision, is_dir)

    json_files = sorted(
        f for f in (os.path.join(args.json_dir, p) for p in os.listdir(args.json_dir))
        if os.path.isfile(f) and f.lower().endswith('.json')
    )
    if args.verbose:
        print(f"[Batch] json files found = {len(json_files)}")

    miss_video, done, fail = 0, 0, 0
    for jpath in json_files:
        base = _basename_no_ext(jpath)
        stem_rule = _drop_rules_suffix(base)

        vpath, vstem = _best_match_video(args.video_dir, stem_rule)
        if not vpath:
            miss_video += 1
            print(f"[Skip] No video matched for rule '{base}' (stem='{stem_rule}') under {args.video_dir}")
            continue

        # ★ 找断言文件（可选）
        assert_path = _best_match_assert(args.assert_dir, vstem) if args.assert_dir else None
        if args.verbose:
            print(f"[Match] rule={base} -> video={vstem} | assert={'None' if not assert_path else os.path.basename(assert_path)}")

        out_json = os.path.join(args.out_dir, f"{vstem}_VQA.json")

        # 如开启 reload-per-video，每轮开始前加载，结束后释放
        if args.reload_per_video:
            try:
                model, tok = load_model(where, device, dtype, args.cache_dir, args.local_files_only, args.revision, is_dir)
            except Exception as e:
                fail += 1
                print(f"[Error] load model failed before {vstem}: {e}")
                if args.verbose:
                    traceback.print_exc()
                continue

        try:
            _process_one(jpath, vpath, out_json, model, tok, args, assert_path)
            done += 1
        except Exception as e:
            fail += 1
            print(f"[Error] {vstem} <- {base}: {e}")
            if args.verbose:
                traceback.print_exc()
        finally:
            # 循环级别的保险释放：无论成功失败都回收
            try:
                if model is not None:
                    _flush_model_caches(model)
            except Exception:
                pass
            _finalize_gpu_step()

            # reload-per-video 模式下，处理完一个视频立刻卸载模型以归还显存
            if args.reload_per_video and model is not None:
                try:
                    # 断开权重引用
                    del model, tok
                except Exception:
                    pass
                model, tok = (None, None)
                _finalize_gpu_step()

    print(f"[Summary] success={done}, missing_video={miss_video}, failed={fail}")

if __name__ == '__main__':
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')
        os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True,max_split_size_mb:128,garbage_collection_threshold:0.6')
        # 可选：放宽 matmul 精度以降低显存（按需启用）
        try:
            torch.set_float32_matmul_precision('medium')
        except Exception:
            pass
        main()
